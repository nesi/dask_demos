{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters tuning on HPC (advanced)\n",
    "\n",
    "TODO intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO goals:\n",
    "- advanced strategies (hyperband, Bayesian optimization)\n",
    "- long running jobs & partial_fit\n",
    "- large dataset & partial fit\n",
    "- non scikit-learn compatible API (ask/tell)\n",
    "- GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from skorch import NeuralNetClassifier\n",
    "from src.torch_models import SimpleMLP\n",
    "\n",
    "import joblib\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask_ml.preprocessing import MinMaxScaler\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "import dask.array as da\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO start dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(\n",
    "    {\n",
    "        \"distributed.worker.memory.target\": False,  # avoid spilling to disk\n",
    "        \"distributed.worker.memory.spill\": False,  # avoid spilling to disk\n",
    "    }\n",
    ")\n",
    "cluster = SLURMCluster(\n",
    "    cores=10,\n",
    "    processes=2,\n",
    "    memory=\"8GiB\",\n",
    "    walltime=\"0-00:30\",\n",
    "    log_directory=\"../dask/logs\",  # folder for SLURM logs for each worker\n",
    "    local_directory=\"../dask\",  # folder for workers data\n",
    "    queue=\"hugemem\",  # TODO remove\n",
    ")\n",
    "cluster.scale(n=20)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "For testing purpose, use a local Dask cluster to check everything works, for\n",
    "example running few iterations on a smaller dataset, as follows\n",
    "\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client(n_workers=1, processes=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO load mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO show some samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO skorch model (why? scikit-learn API, CPU/GPU, partial fit, mention keras wrapper/issues?)\n",
    "- https://skorch.readthedocs.io/en/stable/user/quickstart.html\n",
    "- https://github.com/skorch-dev/skorch/blob/master/notebooks/MNIST.ipynb\n",
    "- https://github.com/skorch-dev/skorch/blob/master/notebooks/Basic_Usage.ipynb\n",
    "\n",
    "TODO use log_softmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "net = NeuralNetClassifier(\n",
    "    module=SimpleMLP,\n",
    "    module__input_dim=X.shape[1],\n",
    "    module__output_dim=len(np.unique(y)),\n",
    "    module__hidden_dim=50,\n",
    "    module__dropout=0.5,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer__lr=1e-3,\n",
    ")\n",
    "mlp = make_pipeline(MinMaxScaler(), net)\n",
    "_ = mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test)\n",
    "mlp_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Simple MLP test accuracy is {mlp_acc * 100:.2f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO dask-ml grid search (factorize preprocessing), see\n",
    "- https://ml.dask.org/modules/generated/dask_ml.model_selection.GridSearchCV.html\n",
    "- https://ml.dask.org/modules/generated/dask_ml.preprocessing.StandardScaler.html#dask_ml.preprocessing.StandardScaler\n",
    "\n",
    "data as dask-array & float32 (client.scatter), grid search -> standard scaler -> neural net\n",
    "visualize graph!\n",
    "\n",
    "note: could use incrementalCV if data too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"neuralnetclassifier__module__hidden_dim\": [50, 100, 200],\n",
    "    \"neuralnetclassifier__module__dropout\": [0.2, 0.5, 0.8],\n",
    "    \"neuralnetclassifier__optimizer__lr\": [1e-2, 1e-3, 1e-4],\n",
    "}\n",
    "mlp.set_params(neuralnetclassifier__verbose=0)\n",
    "mlp_tuned = GridSearchCV(mlp, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train = da.array(X_train).rechunk({0: 500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "mlp_tuned.fit(X_train, y_train)\n",
    "elapsed = time.perf_counter() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = len(mlp_tuned.cv_results_[\"params\"]) * mlp_tuned.n_splits_\n",
    "print(\n",
    "    f\"Model fitting took {elapsed:0.2f}s (equivalent to {elapsed / n_jobs:0.2f}s \"\n",
    "    \"per model fit on a single node).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tuned = mlp_tuned.predict(X_test)\n",
    "mlp_tuned_acc = accuracy_score(y_test, y_pred_tuned)\n",
    "print(f\"Tuned MLP test accuracy is {mlp_tuned_acc * 100:.2f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best hyper-parameters: {mlp_tuned.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "TODO dask-ml hyperband\n",
    "https://ml.dask.org/hyper-parameter-search.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "TODO ask & tell interfaces (scikit-optimize, Ax service API, nevergrad)\n",
    "- use nevergrad because parallel ask\n",
    "- loop: ask, client.submit cross-validated on preprocess + model, tell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    return loss(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def cross_val_score(model, X, y, scorer):\n",
    "    kf = KFold()\n",
    "    scores = [\n",
    "        score_model(model, X[train], y[train], X[test], y[test])\n",
    "        for train, test in kf.split(X)\n",
    "    ]\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO scikit-optimize BayesSearchCV?\n",
    "\n",
    "TODO nevergrad executor concurrent?\n",
    "\n",
    "TODO switch to GPU training (change dask cluster and skorch?)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "dask_demo",
   "language": "python",
   "name": "dask_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
