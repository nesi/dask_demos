{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters tuning on HPC (basic)\n",
    "\n",
    "This demo illustrates one simple way to adapt a grid search strategy for\n",
    "hyper-parameters tuning to use HPC for the many parallel computations involved.\n",
    "\n",
    "In this example, we will rely on [Dask](https://dask.org) to do the heavy lifting,\n",
    "distributing the parallel operations on SLURM jobs. We'll see how it can be used\n",
    "as a backend for [Scikit-Learn](https://scikit-learn.org) estimators, with very\n",
    "little changes compared to a vanilla grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import joblib\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST data from [OpenML](https://www.openml.org/d/554)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True)\n",
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, train_size=5000, test_size=10000, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a simple multi-layer perceptron neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitting took 14.10s.\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "mlp = MLPClassifier().fit(X_train, y_train)\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Model fitting took {elapsed:0.2f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MLP test accuracy is 94.25%.\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlp.predict(X_test)\n",
    "mlp_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Baseline MLP test accuracy is {mlp_acc * 100:.2f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyper-parameters using a random search strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (200,)],\n",
    "    \"alpha\": np.logspace(-5, -3, 3),\n",
    "    \"learning_rate_init\": np.logspace(-4, -2, 3),\n",
    "}\n",
    "mlp_tuned = GridSearchCV(MLPClassifier(), param_grid, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a Dask cluster using SLURM jobs as workers.\n",
    "\n",
    "There are a couple of things we need to configure here:\n",
    "\n",
    "- disabling the mechanism to write on disk when workers run out of memory,\n",
    "- memory, CPUs, maximum time and number of workers per SLURM job,\n",
    "- dask folders for log files and workers data.\n",
    "\n",
    "We recommend putting the log folder and workers data folders in your\n",
    "`/nesi/nobackup/<project_code>` folder, most indicated for temporary files\n",
    "(see [NeSI File Systems and Quotas](https://support.nesi.org.nz/hc/en-gb/articles/360000177256-NeSI-File-Systems-and-Quotas)).\n",
    "\n",
    "All of these options can be set in configuration files, see [Dask configuration](https://docs.dask.org/en/latest/configuration.html)\n",
    "and [Dask jobqueue configuration](https://jobqueue.dask.org/en/latest/configuration-setup.html)\n",
    "for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(\n",
    "    {\n",
    "        \"distributed.worker.memory.target\": False,  # avoid spilling to disk\n",
    "        \"distributed.worker.memory.spill\": False,  # avoid spilling to disk\n",
    "    }\n",
    ")\n",
    "cluster = SLURMCluster(\n",
    "    cores=10,\n",
    "    processes=2,\n",
    "    memory=\"8GiB\",\n",
    "    walltime=\"0-00:30\",\n",
    "    log_directory=\"../dask/logs\",  # folder for SLURM logs for each worker\n",
    "    local_directory=\"../dask\",  # folder for workers data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spawn 20 workers and connect a client to be able use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(n=20)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn uses [Joblib](https://joblib.readthedocs.io) to parallelize\n",
    "computations of many operations, including the randomized search on hyper-parameters.\n",
    "If we configure Joblib to use Dask as a backend, computations will be automatically\n",
    "scheduled and distributed on nodes of the HPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    }
   ],
   "source": [
    "with joblib.parallel_backend(\"dask\", wait_for_workers_timeout=600):\n",
    "    start = time.perf_counter()\n",
    "    mlp_tuned.fit(X_train, y_train)\n",
    "    elapsed = time.perf_counter() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitting took 452.75s (equivalent to 3.35s per model fit on a single node).\n"
     ]
    }
   ],
   "source": [
    "n_jobs = len(mlp_tuned.cv_results_[\"params\"]) * mlp_tuned.n_splits_\n",
    "print(\n",
    "    f\"Model fitting took {elapsed:0.2f}s (equivalent to {elapsed / n_jobs:0.2f}s \"\n",
    "    \"per model fit on a single node).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy an optimized model :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned MLP test accuracy is 95.14%.\n"
     ]
    }
   ],
   "source": [
    "y_pred_tuned = mlp_tuned.predict(X_test)\n",
    "mlp_tuned_acc = accuracy_score(y_test, y_pred_tuned)\n",
    "print(f\"Tuned MLP test accuracy is {mlp_tuned_acc * 100:.2f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'alpha': 0.001, 'hidden_layer_sizes': (200,), 'learning_rate_init': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best hyper-parameters: {mlp_tuned.best_params_}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "dask_demo",
   "language": "python",
   "name": "dask_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
