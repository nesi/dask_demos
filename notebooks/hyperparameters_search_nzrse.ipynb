{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters tuning on HPC - NZ RSE 2020\n",
    "\n",
    "This demo illustrates one simple way multiple ways to adapt a random search\n",
    "strategy for hyper-parameters tuning to use HPC for the many parallel\n",
    "computations involved.\n",
    "\n",
    "In this example, we will rely on [Dask](https://dask.org) to do the heavy lifting,\n",
    "distributing the parallel operations on SLURM jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import joblib\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask_ml.model_selection import HyperbandSearchCV\n",
    "\n",
    "import torch\n",
    "import skorch\n",
    "from hpc_for_datascience_demos.torch_models import SimpleCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST data from [OpenML](https://www.openml.org/d/554)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "X = X / 255.0\n",
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains images of digits. Here is a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 10, figsize=(12, 5))\n",
    "for ax, digit in zip(axes, X):\n",
    "    ax.imshow(digit.reshape(28, 28))\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep this example code quick, let's use only a subset of the whole data set\n",
    "as train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, train_size=5000, test_size=10000, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a simple multi-layer perceptron neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mlp = MLPClassifier(random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test)\n",
    "mlp_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Baseline MLP test accuracy is {mlp_acc * 100:.2f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyper-parameters using a random search strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    \"hidden_layer_sizes\": st.randint(50, 200),\n",
    "    \"alpha\": st.loguniform(1e-5, 1e-2),\n",
    "    \"learning_rate_init\": st.loguniform(1e-4, 1e-1),\n",
    "}\n",
    "mlp_tuned = RandomizedSearchCV(\n",
    "    MLPClassifier(random_state=42), param_space, n_iter=10, random_state=42, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a Dask cluster using SLURM jobs as workers.\n",
    "\n",
    "There are a couple of things we need to configure here:\n",
    "\n",
    "- disabling the mechanism to write on disk when workers run out of memory,\n",
    "- memory, CPUs, maximum time and number of workers per SLURM job,\n",
    "- dask folders for log files and workers data.\n",
    "\n",
    "All of these options can be set in configuration files, see [Dask configuration](https://docs.dask.org/en/latest/configuration.html)\n",
    "and [Dask jobqueue configuration](https://jobqueue.dask.org/en/latest/configuration-setup.html)\n",
    "for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(\n",
    "    {\n",
    "        \"distributed.worker.memory.target\": False,  # avoid spilling to disk\n",
    "        \"distributed.worker.memory.spill\": False,  # avoid spilling to disk\n",
    "    }\n",
    ")\n",
    "cluster = SLURMCluster(\n",
    "    cores=4,\n",
    "    processes=1,\n",
    "    memory=\"4GiB\",\n",
    "    walltime=\"0-00:10\",\n",
    "    log_directory=\"../dask/logs\",  # folder for SLURM logs for each worker\n",
    "    local_directory=\"../dask\",  # folder for workers data\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spawn 10 workers and connect a client to be able use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(n=10)\n",
    "client.wait_for_workers(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn uses [Joblib](https://joblib.readthedocs.io) to parallelize\n",
    "computations of many operations, including the randomized search on\n",
    "hyper-parameters. If we configure Joblib to use Dask as a backend,\n",
    "computations will be automatically scheduled and distributed on nodes of the\n",
    "HPC platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with joblib.parallel_backend(\"dask\", scatter=[X_train, y_train]):\n",
    "    mlp_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy an optimized model :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tuned = mlp_tuned.predict(X_test)\n",
    "mlp_tuned_acc = accuracy_score(y_test, y_pred_tuned)\n",
    "print(f\"Tuned MLP test accuracy is {mlp_tuned_acc * 100:.2f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best hyper-parameters:\\n{json.dumps(mlp_tuned.best_params_, indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first approach requires very little changes but is far from optimal from\n",
    "a ressource usage perspective. The [Dask-ML](https://ml.dask.org/) package\n",
    "implements a more advanced algorithm, [Hyperband](https://arxiv.org/abs/1603.06560),\n",
    "designed to better use a finite bugdet, using early stopping of bad\n",
    "configurations. It relies on Scikit-learn API, assuming the estimator\n",
    "implements the `partial_fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_hyper = HyperbandSearchCV(\n",
    "    MLPClassifier(random_state=42),\n",
    "    param_space,\n",
    "    max_iter=200,\n",
    "    aggressiveness=4,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = mlp_hyper.fit(X_train, y_train, classes=np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hyper = mlp_hyper.predict(X_test)\n",
    "mlp_hyper_acc = accuracy_score(y_test, y_pred_hyper)\n",
    "print(f\"MLP (Hyperband) test accuracy is {mlp_hyper_acc * 100:.2f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best hyper-parameters:\\n{json.dumps(mlp_hyper.best_params_, indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to try some deep learning models trained with GPUs, we need to\n",
    "start a new Dask cluster, requesting the right resources. First let's stop the\n",
    "current cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then start a new cluster, explicitly asking for GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(\n",
    "    cores=4,\n",
    "    processes=1,\n",
    "    memory=\"6GiB\",\n",
    "    walltime=\"0-00:10\",\n",
    "    log_directory=\"../dask/logs\",\n",
    "    local_directory=\"../dask\",\n",
    "    job_extra=[\"--gres gpu:A100 --qos=testing\"],  # passed to job script to request a GPU\n",
    "    env_extra=[\"module load cuDNN/8.0.2.39-CUDA-11.0.2\"], # run before worker start\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use an adaptive scaling strategy, asking Dask scheduler to start at\n",
    "least one worker and letting it spawning on the fly more if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.adapt(minimum=1, maximum=6)\n",
    "client.wait_for_workers(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "For the deep learning part, let's use a simple convolutional neural network\n",
    "implemented using [Pytorch](https://pytorch.org/). We make use of [Skorch](https://github.com/skorch-dev/skorch)\n",
    "to make it in a Scikit-learn compatible estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "cnn = skorch.NeuralNetClassifier(\n",
    "    module=SimpleCNN,\n",
    "    module__input_dims=(28, 28),\n",
    "    module__output_dim=len(np.unique(y)),\n",
    "    module__n_chans=32,\n",
    "    module__hidden_dim=100,\n",
    "    module__dropout=0.5,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer__lr=1e-3,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Skorch, our model is compatible with Scikit-learn API, and thus can\n",
    "be used with Dask-ML meta-estimators. Hence we will use Hyberband to search\n",
    "for the best hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    \"module__n_chans\": st.randint(10, 64),\n",
    "    \"module__hidden_dim\": st.randint(50, 200),\n",
    "    \"module__dropout\": st.uniform(),\n",
    "    \"optimizer__lr\": st.loguniform(1e-4, 1e-1),\n",
    "}\n",
    "mlp_torch = HyperbandSearchCV(cnn, param_space, max_iter=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    mlp_torch.fit(X_train.astype(np.float32), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_torch = mlp_torch.predict(X_test.astype(np.float32))\n",
    "mlp_torch_acc = accuracy_score(y_test, y_pred_torch)\n",
    "print(f\"CNN (PyTorch & Hyperband) test accuracy is {mlp_torch_acc * 100:.2f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best hyper-parameters:\\n{json.dumps(mlp_torch.best_params_, indent=4)}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "dask_demos",
   "language": "python",
   "name": "dask_demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
